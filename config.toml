[db]
# path relative from _internal folder
path = "../db"

[llm]
[llm.completion]
# path relative from executable. must be of "gguf" type
model = "../<model>.gguf"

# refer to model card for prompting format used by model
# valid values: CHATML, GEMMA, LLAMA
prompt_format = "<prompt format>"

# optionals:
# does model require flash attention? default is false
#flash_attention = true

# input context size. default is 0 (max supported size)
# need to set manually if size is too large to fit in available VRAM
#context_size = 0

[llm.embedding]
# path relative from executable. must be of "gguf" type and for embedding use
model = "../<model>.gguf"

[document]
# boundary between paragraphs and chapters
separator = "<separator>"

# document's written script. valid values: LATIN, HANZI
script = "<script>"

[document.chunk]
# number of words per chunk. for latin alphabet documents, is at half of embedding model's token limit
size = <#>

# percent of words that overlap between chunks. between 0 (no overlap) to 0.25
overlap = <#>
